version: '3.8'
services:
  # Core API service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - type: bind
        source: ./src
        target: /app/src
      - type: volume
        source: chroma_data
        target: /app/chroma_data
      - type: bind
        source: ./data
        target: /app/data
      # Add Docker socket mount for host machine deployment
      - /var/run/docker.sock:/var/run/docker.sock
      # Add shared volume for environments
      - environments_data:/app/data/environments
    environment:
      - LLM_PROVIDER=ollama
      - LLM_MODEL=llama3
      - LLM_API_BASE=http://172.17.0.1:11434
      - CHROMA_DB_PATH=/app/chroma_data
      - PYTHONPATH=/app
      # Add environment variable for host deployment
      - HOST_DEPLOYMENT=true
    networks:
      - infra_network
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Add this for GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Make GPU runtime optional
    runtime: ${GPU_RUNTIME:-runc}
    
  # React UI service
  react-ui:
    build:
      context: ./ui/react-ui
      dockerfile: Dockerfile
    image: infra-automation-react-ui
    container_name: infra-automation-react-ui
    ports:
      - "80:80"
    depends_on:
      - api
    networks:
      - infra_network
    restart: unless-stopped
    environment:
      - REACT_APP_API_URL=http://192.168.5.199:8000
      
  # Test service for running automated tests
  test:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - type: bind
        source: ./src
        target: /app/src
      - type: bind
        source: ./test-results
        target: /app/test-results
    environment:
      - LLM_PROVIDER=ollama
      - LLM_MODEL=llama3
      - LLM_API_BASE=http://172.17.0.1:11434
      - TESTING=1
      - PYTHONPATH=/app
    networks:
      - infra_network
    command: python -m src.tests.run_tests
    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  infra_network:
    driver: bridge
    
volumes:
  chroma_data:
    driver: local
  # Add volume for environments
  environments_data:
    driver: local